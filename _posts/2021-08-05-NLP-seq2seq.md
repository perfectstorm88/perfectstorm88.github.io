

# 要点说明

# bert源码解读

https://github.com/DA-southampton/Read_Bert_Code

- ：

# 通过docker运行tensorflow

https://hub.docker.com/r/tensorflow/tensorflow

docker pull tensorflow/tensorflow:2.2.3-gpu-jupyter

docker run -it --runtime=nvidia   --rm -v $(realpath ~/notebooks):/tf/notebooks -p 8091:8888 tensorflow/tensorflow:2.2.3-gpu-jupyter

# 参考
- [基于全词遮罩（Whole Word Masking）技术的中文预训练模型BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)
- [苏剑林的更清晰、更轻量级的keras版bert](https://github.com/bojone/bert4keras), 3.5K star
- [30分钟带你彻底掌握Bert源码(Pytorch)，超详细！！不看后悔！！](https://zhuanlan.zhihu.com/p/148062852)
- [Keras vs PyTorch](https://www.geeksforgeeks.org/keras-vs-pytorch/):
  - 在api、调试、速度方便比较了下，感觉pytorch更好一点儿
- [BERT模型TensorFlow与Pytorch互相转换](https://blog.csdn.net/yaohaishen/article/details/117467889):